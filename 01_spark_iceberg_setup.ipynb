{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Apache Iceberg with Spark (Local Hadoop Catalog)\n",
        "\n",
        "This notebook sets up a local Apache Iceberg data lake using Spark with a Hadoop catalog that writes metadata and data files to your local filesystem.\n",
        "\n",
        "- Catalog name: `local`\n",
        "- Warehouse path: `/Users/aliceguo/src/apache-iceberg-for-dummy/warehouse`\n",
        "- No external services (no Hive Metastore, no MinIO) needed\n",
        "\n",
        "You'll:\n",
        "- Install dependencies\n",
        "- Start a SparkSession with Iceberg runtime jars\n",
        "- Create a namespace and table\n",
        "- Write and read data using Iceberg\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os\n",
        "python_exe = sys.executable  # driver Python (3.12)\n",
        "\n",
        "os.environ[\"PYSPARK_PYTHON\"] = python_exe\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = python_exe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "- Python 3.12\n",
        "- Jupyter (`pip install notebook jupyterlab`)\n",
        "- PySpark installed in this environment\n",
        "\n",
        "If PySpark isn't installed, the cell below will install it alongside pandas and pyarrow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.12 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'3.5.1'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pip install -q pyspark==3.5.1 pandas pyarrow certifi\n",
        "# then re-run the cell:\n",
        "import pyspark; pyspark.__version__\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure Spark with Iceberg\n",
        "\n",
        "We download Iceberg runtime jars matching your Spark version and launch a SparkSession pre-configured for a local Hadoop catalog.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Spark 3.5.1 with Iceberg runtime for 3.5\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'/Users/aliceguo/src/apache-iceberg-for-dummy/.jars/iceberg-spark-runtime-3.5_2.12-1.6.1.jar'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "WAREHOUSE = \"/Users/aliceguo/src/apache-iceberg-for-dummy/warehouse\"\n",
        "Path(WAREHOUSE).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Resolve Spark version at runtime\n",
        "import pyspark\n",
        "spark_version = pyspark.__version__\n",
        "scala_suffix = \"2.12\"  # Spark 3.x uses Scala 2.12\n",
        "\n",
        "# Choose Iceberg runtime version compatible with Spark 3.3/3.4/3.5\n",
        "iceberg_version = os.environ.get(\"ICEBERG_VERSION\", \"1.6.1\")\n",
        "\n",
        "JARS_DIR = Path(\"/Users/aliceguo/src/apache-iceberg-for-dummy/.jars\")\n",
        "JARS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "base = \"https://repo1.maven.org/maven2\"\n",
        "minor = \".\".join(spark_version.split(\".\")[:2])\n",
        "supported_minors = {\"3.5\", \"3.4\", \"3.3\"}\n",
        "runtime_minor = minor if minor in supported_minors else \"3.5\"\n",
        "artifact_group = f\"org/apache/iceberg/iceberg-spark-runtime-{runtime_minor}_{scala_suffix}\"\n",
        "artifact_file = f\"{iceberg_version}/iceberg-spark-runtime-{runtime_minor}_{scala_suffix}-{iceberg_version}.jar\"\n",
        "\n",
        "# Download runtime jar if missing\n",
        "jar_dest = JARS_DIR / artifact_file.split(\"/\")[-1]\n",
        "if not jar_dest.exists():\n",
        "    url = f\"{base}/{artifact_group}/{artifact_file}\"\n",
        "    print(\"Downloading\", url)\n",
        "    urlretrieve(url, jar_dest)\n",
        "\n",
        "extra_jars = \",\".join(str(p) for p in JARS_DIR.glob(\"*.jar\"))\n",
        "print(\"Using Spark\", spark_version, \"with Iceberg runtime for\", runtime_minor)\n",
        "extra_jars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "WAREHOUSE = \"/Users/aliceguo/src/apache-iceberg-for-dummy/warehouse\"\n",
        "Path(WAREHOUSE).mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'3.5.1'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder.appName(\"iceberg-local-hadoop\")\n",
        "    .config(\n",
        "        \"spark.sql.extensions\",\n",
        "        \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
        "    )\n",
        "    .config(\"spark.jars\", extra_jars)\n",
        "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
        "    .config(\"spark.sql.catalog.local.type\", \"hadoop\")\n",
        "    .config(\"spark.sql.catalog.local.warehouse\", WAREHOUSE)\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark.version\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create namespace and table\n",
        "\n",
        "We'll use catalog `local` and namespace `demo`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "|demo     |events   |false      |\n",
            "+---------+---------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS local.demo\")\n",
        "spark.sql(\"DROP TABLE IF EXISTS local.demo.events\")\n",
        "spark.sql(\n",
        "    \"\"\"\n",
        "    CREATE TABLE local.demo.events (\n",
        "      id BIGINT,\n",
        "      category STRING,\n",
        "      ts TIMESTAMP\n",
        "    ) USING iceberg\n",
        "    PARTITIONED BY (days(ts))\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "spark.sql(\"SHOW TABLES IN local.demo\").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------+-------------------+\n",
            "| id|category|                 ts|\n",
            "+---+--------+-------------------+\n",
            "|  1|   alpha|2024-01-01 10:00:00|\n",
            "|  2|    beta|2024-01-02 12:30:00|\n",
            "|  3|   alpha|2024-01-02 13:45:00|\n",
            "+---+--------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "rows = [\n",
        "    (1, \"alpha\", datetime(2024, 1, 1, 10, 0, 0)),\n",
        "    (2, \"beta\", datetime(2024, 1, 2, 12, 30, 0)),\n",
        "    (3, \"alpha\", datetime(2024, 1, 2, 13, 45, 0)),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(rows, [\"id\", \"category\", \"ts\"]) \n",
        "\n",
        "df.writeTo(\"local.demo.events\").append()\n",
        "\n",
        "spark.table(\"local.demo.events\").orderBy(\"id\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "row_count: 0\n"
          ]
        }
      ],
      "source": [
        "# Table statistics for local.demo.events\n",
        "\n",
        "# Row count from the table\n",
        "row_count = spark.table(\"local.demo.events\").count()\n",
        "print(f\"row_count: {row_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+-------------------+-----------+\n",
            "|num_data_files|total_rows_in_files|total_bytes|\n",
            "+--------------+-------------------+-----------+\n",
            "|0             |NULL               |NULL       |\n",
            "+--------------+-------------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Data files summary (count, total rows, total bytes)\n",
        "spark.sql(\n",
        "    \"\"\"\n",
        "    SELECT\n",
        "      count(*)                AS num_data_files,\n",
        "      sum(record_count)       AS total_rows_in_files,\n",
        "      sum(file_size_in_bytes) AS total_bytes\n",
        "    FROM local.demo.events.files\n",
        "    \"\"\"\n",
        ").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+---------------+------------------------+\n",
            "|partition|spec_id|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|last_updated_at|last_updated_snapshot_id|\n",
            "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+---------------+------------------------+\n",
            "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+---------------+------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Partition-level stats (if partitioned)\n",
        "spark.sql(\"SELECT * FROM local.demo.events.partitions\").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+---------+-------+\n",
            "|col_name           |data_type|comment|\n",
            "+-------------------+---------+-------+\n",
            "|made_current_at    |timestamp|NULL   |\n",
            "|snapshot_id        |bigint   |NULL   |\n",
            "|parent_id          |bigint   |NULL   |\n",
            "|is_current_ancestor|boolean  |NULL   |\n",
            "+-------------------+---------+-------+\n",
            "\n",
            "+-------------+------------------+-------+\n",
            "|col_name     |data_type         |comment|\n",
            "+-------------+------------------+-------+\n",
            "|committed_at |timestamp         |NULL   |\n",
            "|snapshot_id  |bigint            |NULL   |\n",
            "|parent_id    |bigint            |NULL   |\n",
            "|operation    |string            |NULL   |\n",
            "|manifest_list|string            |NULL   |\n",
            "|summary      |map<string,string>|NULL   |\n",
            "+-------------+------------------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"DESCRIBE TABLE local.demo.events.history\").show(truncate=False)\n",
        "spark.sql(\"DESCRIBE TABLE local.demo.events.snapshots\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+-----------+---------+-------------------+\n",
            "|made_current_at|snapshot_id|parent_id|is_current_ancestor|\n",
            "+---------------+-----------+---------+-------------------+\n",
            "+---------------+-----------+---------+-------------------+\n",
            "\n",
            "+-----------+---------+------------+---------+\n",
            "|snapshot_id|parent_id|committed_at|operation|\n",
            "+-----------+---------+------------+---------+\n",
            "+-----------+---------+------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# History: when snapshots became current\n",
        "spark.sql(\"\"\"\n",
        "  SELECT made_current_at, snapshot_id, parent_id, is_current_ancestor\n",
        "  FROM local.demo.events.history\n",
        "  ORDER BY made_current_at\n",
        "\"\"\").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------+-------------------+\n",
            "| id|category|                 ts|\n",
            "+---+--------+-------------------+\n",
            "|  4|   gamma|2024-01-03 09:00:00|\n",
            "|  4|   gamma|2024-01-03 09:00:00|\n",
            "|  4|   gamma|2024-01-03 09:00:00|\n",
            "|  4|   gamma|2024-01-03 09:00:00|\n",
            "|  4|   gamma|2024-01-03 09:00:00|\n",
            "+---+--------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"INSERT INTO local.demo.events VALUES (4, 'gamma', timestamp '2024-01-03 09:00:00')\")\n",
        "spark.sql(\"SELECT * FROM local.demo.events ORDER BY id\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-------------------+-----------------------+---------+\n",
            "|snapshot_id        |parent_id          |committed_at           |operation|\n",
            "+-------------------+-------------------+-----------------------+---------+\n",
            "|2120259642793238659|NULL               |2025-10-13 20:30:18.203|append   |\n",
            "|6813048920963727275|2120259642793238659|2025-10-13 20:31:16.44 |append   |\n",
            "|5834103816349075604|6813048920963727275|2025-10-13 20:31:46.413|append   |\n",
            "|8803006309687028399|5834103816349075604|2025-10-13 20:32:08.918|append   |\n",
            "|2816266580316681037|8803006309687028399|2025-10-13 20:32:22.227|append   |\n",
            "+-------------------+-------------------+-----------------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Snapshots: includes committed_at and operation\n",
        "spark.sql(\"\"\"\n",
        "  SELECT snapshot_id, parent_id, committed_at, operation\n",
        "  FROM local.demo.events.snapshots\n",
        "  ORDER BY committed_at\n",
        "\"\"\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------+---+\n",
            "|id |category|ts |\n",
            "+---+--------+---+\n",
            "+---+--------+---+\n",
            "\n",
            "+-------------------+-------------------+-----------------------+---------+\n",
            "|snapshot_id        |parent_id          |committed_at           |operation|\n",
            "+-------------------+-------------------+-----------------------+---------+\n",
            "|5292537171771044943|2816266580316681037|2025-10-13 21:05:01.881|delete   |\n",
            "|2816266580316681037|8803006309687028399|2025-10-13 20:32:22.227|append   |\n",
            "|8803006309687028399|5834103816349075604|2025-10-13 20:32:08.918|append   |\n",
            "|5834103816349075604|6813048920963727275|2025-10-13 20:31:46.413|append   |\n",
            "|6813048920963727275|2120259642793238659|2025-10-13 20:31:16.44 |append   |\n",
            "+-------------------+-------------------+-----------------------+---------+\n",
            "\n",
            "+-----------------------+---------+-------------+---------------+-----------+-------------+\n",
            "|committed_at           |operation|added_records|removed_records|added_files|removed_files|\n",
            "+-----------------------+---------+-------------+---------------+-----------+-------------+\n",
            "|2025-10-13 21:05:01.881|delete   |0            |0              |0          |0            |\n",
            "+-----------------------+---------+-------------+---------------+-----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Delete records and observe snapshot operation change\n",
        "\n",
        "# Delete example: remove all rows with id = 4\n",
        "spark.sql(\"DELETE FROM local.demo.events WHERE id = 4\")\n",
        "\n",
        "# Show current table contents\n",
        "spark.table(\"local.demo.events\").orderBy(\"id\", \"ts\").show(truncate=False)\n",
        "\n",
        "# Show most recent snapshots with operations\n",
        "spark.sql(\n",
        "    \"\"\"\n",
        "    SELECT snapshot_id, parent_id, committed_at, operation\n",
        "    FROM local.demo.events.snapshots\n",
        "    ORDER BY committed_at DESC\n",
        "    LIMIT 5\n",
        "    \"\"\"\n",
        ").show(truncate=False)\n",
        "\n",
        "# Show added/removed counts on the latest snapshot\n",
        "spark.sql(\n",
        "    \"\"\"\n",
        "    SELECT\n",
        "      committed_at,\n",
        "      operation,\n",
        "      CAST(coalesce(summary['added-records'],   '0') AS BIGINT)   AS added_records,\n",
        "      CAST(coalesce(summary['removed-records'], '0') AS BIGINT)   AS removed_records,\n",
        "      CAST(coalesce(summary['added-data-files'],'0') AS BIGINT)   AS added_files,\n",
        "      CAST(coalesce(summary['removed-data-files'],'0') AS BIGINT) AS removed_files\n",
        "    FROM local.demo.events.snapshots\n",
        "    ORDER BY committed_at DESC\n",
        "    LIMIT 1\n",
        "    \"\"\"\n",
        ").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+-----------+\n",
            "|num_files|total_bytes|\n",
            "+---------+-----------+\n",
            "|30       |45038      |\n",
            "+---------+-----------+\n",
            "\n",
            "row_count: 3000\n"
          ]
        }
      ],
      "source": [
        "# Generate many small data files for compaction demo\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "\n",
        "# Keep one shuffle partition so each write produces a single small file\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "def make_batch(batch_idx: int, num_rows: int = 200):\n",
        "    base_ts = datetime(2024, 1, 1) + timedelta(days=batch_idx % 7)\n",
        "    rows = [\n",
        "        (\n",
        "            batch_idx * 1_000_000 + i,\n",
        "            random.choice([\"alpha\", \"beta\", \"gamma\", \"delta\"]),\n",
        "            base_ts + timedelta(minutes=i)\n",
        "        )\n",
        "        for i in range(num_rows)\n",
        "    ]\n",
        "    return spark.createDataFrame(rows, [\"id\", \"category\", \"ts\"]) \n",
        "\n",
        "# Write many small files via multiple appends\n",
        "for b in range(30):  # increase if you want more files\n",
        "    make_batch(b, num_rows=100).coalesce(1).writeTo(\"local.demo.events\").append()\n",
        "\n",
        "# Validate number of files and row count\n",
        "spark.sql(\n",
        "    \"SELECT COUNT(*) AS num_files, SUM(file_size_in_bytes) AS total_bytes FROM local.demo.events.files\"\n",
        ").show(truncate=False)\n",
        "\n",
        "print(\"row_count:\", spark.table(\"local.demo.events\").count())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-------------------+-----------------------+---------+\n",
            "|snapshot_id        |parent_id          |committed_at           |operation|\n",
            "+-------------------+-------------------+-----------------------+---------+\n",
            "|8409446886655369985|6318710171621420987|2025-10-13 21:19:10.05 |append   |\n",
            "|6318710171621420987|7648811580166220859|2025-10-13 21:19:09.71 |append   |\n",
            "|7648811580166220859|8071463243716675358|2025-10-13 21:19:09.209|append   |\n",
            "|8071463243716675358|6464031408296887663|2025-10-13 21:19:08.679|append   |\n",
            "|6464031408296887663|1491900029434595614|2025-10-13 21:19:08.077|append   |\n",
            "+-------------------+-------------------+-----------------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\n",
        "    \"\"\"\n",
        "    SELECT snapshot_id, parent_id, committed_at, operation\n",
        "    FROM local.demo.events.snapshots\n",
        "    ORDER BY committed_at DESC\n",
        "    LIMIT 5\n",
        "    \"\"\"\n",
        ").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------+----------------------+---------------------+-----------------------+\n",
            "|rewritten_data_files_count|added_data_files_count|rewritten_bytes_count|failed_data_files_count|\n",
            "+--------------------------+----------------------+---------------------+-----------------------+\n",
            "|0                         |0                     |0                    |0                      |\n",
            "+--------------------------+----------------------+---------------------+-----------------------+\n",
            "\n",
            "+---------+-----------+\n",
            "|num_files|total_bytes|\n",
            "+---------+-----------+\n",
            "|7        |15357      |\n",
            "+---------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Compact small data files (bin-pack) and inspect results\n",
        "# Rewrite (bin-pack) data files to target size (128MB example)\n",
        "spark.sql(\n",
        "    \"\"\"\n",
        "    CALL local.system.rewrite_data_files(\n",
        "      table => 'local.demo.events',\n",
        "      options => map(\n",
        "        'target-file-size-bytes','134217728',   -- 128MB\n",
        "        'min-input-files','1'\n",
        "      )\n",
        "    )\n",
        "    \"\"\"\n",
        ").show(truncate=False)\n",
        "\n",
        "# Check files and snapshots after compaction\n",
        "spark.sql(\n",
        "    \"SELECT COUNT(*) AS num_files, SUM(file_size_in_bytes) AS total_bytes FROM local.demo.events.files\"\n",
        ").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+-------------+\n",
            "|operation|num_snapshots|\n",
            "+---------+-------------+\n",
            "|append   |35           |\n",
            "|delete   |1            |\n",
            "|replace  |1            |\n",
            "+---------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Count snapshots by operation type\n",
        "spark.sql(\n",
        "    \"\"\"\n",
        "    SELECT operation, COUNT(*) AS num_snapshots\n",
        "    FROM local.demo.events.snapshots\n",
        "    GROUP BY operation\n",
        "    ORDER BY operation\n",
        "    \"\"\"\n",
        ").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------+---------+-----------+-------------+-------------+---------------+\n",
            "|committed_at           |operation|added_files|removed_files|added_records|removed_records|\n",
            "+-----------------------+---------+-----------+-------------+-------------+---------------+\n",
            "|2025-10-13 20:30:18.203|append   |1          |0            |1            |0              |\n",
            "|2025-10-13 20:31:16.44 |append   |1          |0            |1            |0              |\n",
            "|2025-10-13 20:31:46.413|append   |1          |0            |1            |0              |\n",
            "|2025-10-13 20:32:08.918|append   |1          |0            |1            |0              |\n",
            "|2025-10-13 20:32:22.227|append   |1          |0            |1            |0              |\n",
            "|2025-10-13 21:05:01.881|delete   |0          |0            |0            |0              |\n",
            "|2025-10-13 21:18:56.601|append   |1          |0            |100          |0              |\n",
            "|2025-10-13 21:18:57.028|append   |1          |0            |100          |0              |\n",
            "|2025-10-13 21:18:57.411|append   |1          |0            |100          |0              |\n",
            "|2025-10-13 21:18:57.798|append   |1          |0            |100          |0              |\n",
            "|2025-10-13 21:18:58.195|append   |1          |0            |100          |0              |\n",
            "|2025-10-13 21:18:58.594|append   |1          |0            |100          |0              |\n",
            "|2025-10-13 21:18:58.963|append   |1          |0            |100          |0              |\n",
            "|2025-10-13 21:18:59.401|append   |1          |0            |100          |0              |\n",
            "|2025-10-13 21:18:59.943|append   |1          |0            |100          |0              |\n",
            "|2025-10-13 21:19:00.474|append   |1          |0            |100          |0              |\n",
            "|2025-10-13 21:19:01.001|append   |1          |0            |100          |0              |\n",
            "|2025-10-13 21:19:01.452|append   |1          |0            |100          |0              |\n",
            "|2025-10-13 21:19:01.852|append   |1          |0            |100          |0              |\n",
            "|2025-10-13 21:19:02.214|append   |1          |0            |100          |0              |\n",
            "+-----------------------+---------+-----------+-------------+-------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Per-snapshot added/removed files and records (from snapshots.summary map)\n",
        "spark.sql(\n",
        "    \"\"\"\n",
        "    SELECT\n",
        "      committed_at,\n",
        "      operation,\n",
        "      CAST(coalesce(summary['added-data-files'],   '0') AS BIGINT) AS added_files,\n",
        "      CAST(coalesce(summary['removed-data-files'], '0') AS BIGINT) AS removed_files,\n",
        "      CAST(coalesce(summary['added-records'],      '0') AS BIGINT) AS added_records,\n",
        "      CAST(coalesce(summary['removed-records'],    '0') AS BIGINT) AS removed_records\n",
        "    FROM local.demo.events.snapshots\n",
        "    ORDER BY committed_at\n",
        "    \"\"\"\n",
        ").show(truncate=False)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
